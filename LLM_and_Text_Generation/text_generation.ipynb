{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173c2358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriramramesh/miniconda3/envs/drl_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63065b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f96739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb331f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = 'data/shakespeare_small.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9e817",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed7905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d60bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of text\n",
    "def normalize_text(text: str) -> str:\n",
    "    normalized_text = text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b2bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us kill him, and we'll have corn at our own price.\n",
      "is't a verdict?\n",
      "\n",
      "all:\n",
      "no more talking on't; let it be done: away, away!\n",
      "\n",
      "second citizen:\n",
      "one word, good citizens.\n",
      "\n",
      "first citizen:\n",
      "we are accounted poor\n"
     ]
    }
   ],
   "source": [
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a773800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728ca61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76b321e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text = normalize_text(text)\n",
    "    pretokenized_text = pretokenize_text(normalized_text)\n",
    "    return pretokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a1f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c346f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing the text\n",
    "# We will skip since the sentence doesn't have any special tokens we want to consider for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ada0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode (Tokens to Integer IDs)\n",
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a107bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 50,086 characters\n",
      "Number of unique tokens: 37\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')\n",
    "print(f'Number of unique tokens: {n_tokens:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "304908ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3909d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ec3e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text generation\n",
    "def generate_text_by_char(\n",
    "        input_str: str,\n",
    "        model,\n",
    "        token_mapping: TokenMapping = character_mapping,\n",
    "        num_chars: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses the character based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(tokenized_text=(tokenized_text + generated_tokens),\n",
    "                              model=model,\n",
    "                              token_mapping=token_mapping,\n",
    "                              temperature=temperature,\n",
    "                              topk=topk,\n",
    "                              device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd5aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.5301661688679706\n",
      "[00m 1.8s (0 0.0) 2.1864]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bee and vevenptury havir\n",
      "if:\n",
      "comums mey:\n",
      "oucidy\n",
      " irastr mavif,\n",
      "met to gow ow, it!citisere-TOKEN_NOT_FOUNDctim thongi\n",
      "Epoch 2/25, Loss: 2.181257638154319\n",
      "[00m 3.4s (1 4.0) 1.9870]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bem fiese have an, ato shind thai daraster harfe uss peiind aftorfer;\n",
      "hins, inius:\n",
      "ine deart haet of o\n",
      "Epoch 3/25, Loss: 2.0785382084191415\n",
      "[00m 4.8s (2 8.0) 1.8849]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bevis sinsnores\n",
      "in coak foar.\n",
      "there cont rye sel cot cim andp, gry llott hal and aliendry no; pirse mo\n",
      "Epoch 4/25, Loss: 2.019214431774883\n",
      "[00m 6.3s (3 12.0) 1.8198]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be thies and the bericenyly he 'ut nopher iaret; do ltim uncius:iatrris sotry cofa'linius:\n",
      "olonger pat\n",
      "Epoch 5/25, Loss: 1.976692758657681\n",
      "[00m 7.7s (4 16.0) 1.7796]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to beint heserusss art\n",
      "the, hond ashing\n",
      "prechak covtod. foud.\n",
      "\n",
      "ises\n",
      "the pratity, aloves,\n",
      "the mo:\n",
      "such wor\n",
      "Epoch 6/25, Loss: 1.9433612474618247\n",
      "[00m 9.2s (5 20.0) 1.7545]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bet turem he dow, will wruprome but pithte, wischot haerest, aplderp terus:\n",
      "we pups and lidon.\n",
      "\n",
      "with p\n",
      "Epoch 7/25, Loss: 1.9164561936649651\n",
      "[00m 10.7s (6 24.0) 1.7338]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to beir the suces comes our fice\n",
      "lese he bald withis the grution the depons atpliantry vrat, your for ed \n",
      "Epoch 8/25, Loss: 1.8943732171774672\n",
      "[00m 12.1s (7 28.000000000000004) 1.7176]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bet horeto, wond hond he,\n",
      "plard you parscore bean:\n",
      "yet him no love poven here shownow?\n",
      "hact troth be s\n",
      "Epoch 9/25, Loss: 1.8757061641437176\n",
      "[00m 13.7s (8 32.0) 1.7068]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be t.\n",
      "\n",
      "first ito wheleqee\n",
      "you a\n",
      "knor hubleoger therie for, their huth im,\n",
      "apcond deest nor eas.\n",
      "\n",
      "menen\n",
      "Epoch 10/25, Loss: 1.8597338855457002\n",
      "[00m 15.2s (9 36.0) 1.6995]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be\n",
      "fort sheren otty dreed betor exves ditizens\n",
      "to jindes, grtith on to see,\n",
      "forth temegs ince bave to \n",
      "Epoch 11/25, Loss: 1.8458671189725588\n",
      "[00m 16.8s (10 40.0) 1.6941]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to beirs, anded be gon bus unders pites, lovore and titcusn gook suctighon. hin\n",
      "at me of my esterang and \n",
      "Epoch 12/25, Loss: 1.833766489668776\n",
      "[00m 18.8s (11 44.0) 1.6897]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to beserot, terul this knower for so the, of wratel.\n",
      "\n",
      "bul worf know at again broughll fith\n",
      "thopvatu, the \n",
      "Epoch 13/25, Loss: 1.8230771452474137\n",
      "[00m 21.2s (12 48.0) 1.6847]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to belkgol!\n",
      "\n",
      "distrects plakat thiceady gright and pet and his and their prerbe cas mite qepore ofry, pret\n",
      "Epoch 14/25, Loss: 1.813565875699345\n",
      "[00m 22.8s (13 52.0) 1.6784]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to betortus, 'thich its foel hent to s'le\n",
      "it could\n",
      "cootle\n",
      "you suceng.\n",
      "\n",
      "maved the masing in: iwtl. powf th\n",
      "Epoch 15/25, Loss: 1.8051008398921344\n",
      "[00m 24.2s (14 56.00000000000001) 1.6714]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be coumby\n",
      "thear blusted.\n",
      "\n",
      "merpromus: whong.\n",
      "\n",
      "coriolia:\n",
      "i tike thetcen a cam, to with ofnes\n",
      "in, suction\n",
      "Epoch 16/25, Loss: 1.7974950486478714\n",
      "[00m 26.1s (15 60.0) 1.6645]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to becenue, bumswell fraders, nother! to ca por treefeedence him,\n",
      "i the the marcim was a musted and for s\n",
      "Epoch 17/25, Loss: 1.7906137915845877\n",
      "[00m 28.0s (16 64.0) 1.6591]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be no' ear the apy the vourse\n",
      "of and and flay olal this therhe praven it this heporter;, is by trummar\n",
      "Epoch 18/25, Loss: 1.7843555276005414\n",
      "[00m 29.6s (17 68.0) 1.6553]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be the ladk so with, come;\n",
      "ficeind tipopticificore to oarticia\n",
      "ricinily of, when an\n",
      "and the mus cordit\n",
      "Epoch 19/25, Loss: 1.7786364150123475\n",
      "[00m 31.1s (18 72.0) 1.6520]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to besteed their solia:\n",
      "the and,\n",
      "in withes seleld that his at hond inged prithink\n",
      "for at hargutius home\n",
      "d\n",
      "Epoch 20/25, Loss: 1.773381735950994\n",
      "[00m 32.7s (19 76.0) 1.6490]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bent thiunst could he its, a knone,\n",
      "him sucby's knot prodle.\n",
      "\n",
      "knot; thund eveeds letientitiuld a mome \n",
      "Epoch 21/25, Loss: 1.7685499228608494\n",
      "[00m 34.2s (20 80.0) 1.6456]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to besest word\n",
      "to entwetb, of his.\n",
      "\n",
      "marcius, no sird: to spetter time 'twight velrugition. for, no woulf \n",
      "Epoch 22/25, Loss: 1.7640736323195143\n",
      "[00m 35.6s (21 84.0) 1.6423]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be\n",
      "and rome?\n",
      "\n",
      "nand shoutty'd of beir the coriinius:\n",
      "hate have sever staths to the to tes. one thoid\n",
      "sh\n",
      "Epoch 23/25, Loss: 1.7598774832277633\n",
      "[00m 37.1s (22 88.0) 1.6401]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to be he husool.\n",
      "\n",
      "laty\n",
      "hove\n",
      "a to kfigh, by see poif trops cheat\n",
      "and this trutuss,\n",
      "with to gods\n",
      "you. por e\n",
      "Epoch 24/25, Loss: 1.7559937819886131\n",
      "[00m 38.5s (23 92.0) 1.6381]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bed juted,\n",
      "ifs to stimns\n",
      "at har.\n",
      "\n",
      "mareslen we a pravere, being hicus:\n",
      "'thin we besely'll than marciats\n",
      "Epoch 25/25, Loss: 1.752387052450698\n",
      "[00m 39.9s (24 96.0) 1.6368]\n",
      "------------------------------------------------------------------------\n",
      "to be, or not to bet they sconblew here of and fare, curpen the margith,\n",
      "i be heebe\n",
      "coriol: afficiong bte carintwel\n",
      "to \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "TEST_PHRASE = 'To be, or not to be'\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe51131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the trained model\n",
    "print('Generated text:')\n",
    "op = generate_text_by_char(\n",
    "    input_str=TEST_PHRASE,\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168f1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ffc810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "552a56a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 13,139 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c38b742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "856120f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ae5f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005914dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.524220229358208\n",
      "[00m 0.7s (0 0.0) 5.5130]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be themselves, underli, the in!h lead best?. pound : he the five freedom marc :. four pray him body, so not v\n",
      "Epoch 2/25, Loss: 5.837869755814715\n",
      "[00m 1.5s (1 4.0) 4.9805]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bemity yet though, even, my in o to great forrd? menus no promise take s part so - turnus where ' guess - to\n",
      "Epoch 3/25, Loss: 5.534513481651865\n",
      "[00m 2.1s (2 8.0) 4.5549]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be - know and ; sin, old bold daughter good ye to liketino now my change ' it meural neither south. br years nor some sicidi\n",
      "Epoch 4/25, Loss: 5.283198249630812\n",
      "[00m 2.6s (3 12.0) 4.3330]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be armies know br menenius wallsius ratesus he worship worerti nor : that ; this be slip tovent sinking these he wouldrd silence centuries\n",
      "Epoch 5/25, Loss: 5.0876417799693785\n",
      "[00m 3.2s (4 16.0) 4.1505]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be youth and citizen the bo pre some is find.inius : heaves : deliver ' dd?. com : what what do strike, an\n",
      "Epoch 6/25, Loss: 4.922875266540341\n",
      "[00m 3.7s (5 20.0) 3.9944]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be, marcius ab have instruction! - dissolved mum coriolan him, than delivered a bodily after parties to humorous us to thankh ' s at\n",
      "Epoch 7/25, Loss: 4.7783217203326345\n",
      "[00m 4.2s (6 24.0) 3.8569]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be, thy which a objectces, and n the people. bridius : i ' s timeve the fires, the city mineed sc yours\n",
      "Epoch 8/25, Loss: 4.647646873171737\n",
      "[00m 4.8s (7 28.000000000000004) 3.7379]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be! what ' tis crank, and suce and a crooked inherited cry as can said, the prettyenius : romes super? brutus to\n",
      "Epoch 9/25, Loss: 4.527631353168953\n",
      "[00m 5.3s (8 32.0) 3.6346]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be proud of the cruel, out he when the rest ready the question and the godsnder, let with andnc good willst of rome us more a\n",
      "Epoch 10/25, Loss: 4.416339296247901\n",
      "[00m 5.8s (9 36.0) 3.5417]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be. apply marc in seven pretty,ane not execution : ' s time together have rome, when titus ourianulationsp, indeed, ' d.\n",
      "Epoch 11/25, Loss: 4.312364501487918\n",
      "[00m 6.3s (10 40.0) 3.4568]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be plucked, and will forget digest neighbours. menenius : i south yourselves : good you. volumnia : then i go of beg - honours\n",
      "Epoch 12/25, Loss: 4.214818534618471\n",
      "[00m 6.9s (11 44.0) 3.3791]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be to findhin b. god them the made. menenius : give. gentleus, i letsto them, that within hand coriolan\n",
      "Epoch 13/25, Loss: 4.12287744138299\n",
      "[00m 7.4s (12 48.0) 3.3074]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be vulgar vale la deserves ambitious himpi. in f eyes against the commons in you from took in an upon of wounded, before : coriolanus\n",
      "Epoch 14/25, Loss: 4.035962972989896\n",
      "[00m 8.0s (13 52.0) 3.2407]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be something menenius : my view, should are you calling spoon than, and you live ch may thy places, anest, as heart they\n",
      "Epoch 15/25, Loss: 3.9538795453746145\n",
      "[00m 8.6s (14 56.00000000000001) 3.1793]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be blood in this em heels. messenger : o ', that hunger cannot ; shall be content a ladybri army where a hare him ; i will a\n",
      "Epoch 16/25, Loss: 3.876161497976722\n",
      "[00m 9.1s (15 60.0) 3.1229]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be told likely him ; he did the enemy is bear a bribe princelyted : party saw no, veryamo necks great both : i amterers,\n",
      "Epoch 17/25, Loss: 3.802454257592922\n",
      "[00m 9.7s (16 64.0) 3.0724]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be our achieving of occasion to us. work wears out his trophy than obey? volumnia : a man, by all titus lad : pray upon\n",
      "Epoch 18/25, Loss: 3.7323949325375443\n",
      "[00m 10.2s (17 68.0) 3.0276]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be fright seen him? messenger : he is mine, can be like his which the know of their arts, they know is as look, and loves the\n",
      "Epoch 19/25, Loss: 3.6656547534756543\n",
      "[00m 10.7s (18 72.0) 2.9870]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be - which. menenius : noble sort wear, in him tell shall these din to take the others he hate shall here ; and brow not\n",
      "Epoch 20/25, Loss: 3.6019301868066553\n",
      "[00m 11.3s (19 76.0) 2.9494]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be content to see him! our whole this lend the repuls, and proud the third lady. cominius : sown - - thou right much\n",
      "Epoch 21/25, Loss: 3.5409309410467382\n",
      "[00m 11.8s (20 80.0) 2.9155]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be made being with the cat have let. first madam : thou wash each as coriolanus marcius, have you, ifgn, whichder\n",
      "Epoch 22/25, Loss: 3.4824051851179543\n",
      "[00m 12.4s (21 84.0) 2.8831]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be their ancient people ' s hail than look.. virgilinius : o thee where we are most swordsd stand you, when they yet not the\n",
      "Epoch 23/25, Loss: 3.4263416435660385\n",
      "[00m 12.9s (22 88.0) 2.8520]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be her baby first very by it! now, lies a kind - hardin worship : down. no, and verykersomeveld report. come,\n",
      "Epoch 24/25, Loss: 3.372543323330763\n",
      "[00m 13.4s (23 92.0) 2.8216]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be bold my then shall enuls. summon them lesser a confirmed deal. marcius not according to chain our fabric, and out that they say word,\n",
      "Epoch 25/25, Loss: 3.320919711415361\n",
      "[00m 13.9s (24 96.0) 2.7929]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be? i think to it him, bulk especially for sinking under r the sudden re curb while me! nay, my you is against the commonments i\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fa73de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be them of the people, and the gods doom him. sicinius : i ' ll beat, i am glad, and the gods crown. sic\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=0.1,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fae986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
